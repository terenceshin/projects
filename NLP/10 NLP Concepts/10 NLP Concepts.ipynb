{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://www.dezyre.com/article/10-nlp-techniques-every-data-scientist-should-know/415\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "df = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: guykuo@carson.u.washington.edu (Guy Kuo)\n",
      "Subject: SI Clock Poll - Final Call\n",
      "Summary: Final call for SI clock reports\n",
      "Keywords: SI,acceleration,clock,upgrade\n",
      "Article-I.D.: shelley.1qvfo9INNc3s\n",
      "Organization: University of Washington\n",
      "Lines: 11\n",
      "NNTP-Posting-Host: carson.u.washington.edu\n",
      "\n",
      "A fair number of brave souls who upgraded their SI clock oscillator have\n",
      "shared their experiences for this poll. Please send a brief message detailing\n",
      "your experiences with the procedure. Top speed attained, CPU rated speed,\n",
      "add on cards and adapters, heat sinks, hour of usage per day, floppy disk\n",
      "functionality with 800 and 1.4 m floppies are especially requested.\n",
      "\n",
      "I will be summarizing in the next two days, so please add to the network\n",
      "knowledge base if you have done the clock upgrade and haven't answered this\n",
      "poll. Thanks.\n",
      "\n",
      "Guy Kuo <guykuo@u.washington.edu>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From  guykuo carson u washington edu  Guy Kuo  Subject  SI Clock Poll   Final Call Summary  Final call for SI clock reports Keywords  SI acceleration clock upgrade Article I D   shelley  qvfo INNc s Organization  University of Washington Lines     NNTP Posting Host  carson u washington edu  A fair number of brave souls who upgraded their SI clock oscillator have shared their experiences for this poll  Please send a brief message detailing your experiences with the procedure  Top speed attained  CPU rated speed  add on cards and adapters  heat sinks  hour of usage per day  floppy disk functionality with     and     m floppies are especially requested   I will be summarizing in the next two days  so please add to the network knowledge base if you have done the clock upgrade and haven t answered this poll  Thanks   Guy Kuo  guykuo u washington edu  \n"
     ]
    }
   ],
   "source": [
    "def clean_str(string):\n",
    "    string = re.sub(r\"\\\\n\", \"\", string) # removes new-line characters\n",
    "    string = re.sub(r\"[^A-Za-z]\", \" \", string) # removes numbers, symbols\n",
    "    return string\n",
    "\n",
    "cleaned_text = []\n",
    "\n",
    "for text in df.data:\n",
    "    cleaned_text.append(clean_str(text))\n",
    "    \n",
    "print(cleaned_text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From', 'guykuo', 'carson', 'u', 'washington', 'edu', 'Guy', 'Kuo', 'Subject', 'SI', 'Clock', 'Poll', 'Final', 'Call', 'Summary', 'Final', 'call', 'for', 'SI', 'clock', 'reports', 'Keywords', 'SI', 'acceleration', 'clock', 'upgrade', 'Article', 'I', 'D', 'shelley', 'qvfo', 'INNc', 's', 'Organization', 'University', 'of', 'Washington', 'Lines', 'NNTP', 'Posting', 'Host', 'carson', 'u', 'washington', 'edu', 'A', 'fair', 'number', 'of', 'brave', 'souls', 'who', 'upgraded', 'their', 'SI', 'clock', 'oscillator', 'have', 'shared', 'their', 'experiences', 'for', 'this', 'poll', 'Please', 'send', 'a', 'brief', 'message', 'detailing', 'your', 'experiences', 'with', 'the', 'procedure', 'Top', 'speed', 'attained', 'CPU', 'rated', 'speed', 'add', 'on', 'cards', 'and', 'adapters', 'heat', 'sinks', 'hour', 'of', 'usage', 'per', 'day', 'floppy', 'disk', 'functionality', 'with', 'and', 'm', 'floppies', 'are', 'especially', 'requested', 'I', 'will', 'be', 'summarizing', 'in', 'the', 'next', 'two', 'days', 'so', 'please', 'add', 'to', 'the', 'network', 'knowledge', 'base', 'if', 'you', 'have', 'done', 'the', 'clock', 'upgrade', 'and', 'haven', 't', 'answered', 'this', 'poll', 'Thanks', 'Guy', 'Kuo', 'guykuo', 'u', 'washington', 'edu']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized = []\n",
    "\n",
    "for article in cleaned_text:\n",
    "    tokenized.append(word_tokenize(article))\n",
    "    \n",
    "print(tokenized[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer\n",
      "Root word for cats --> cat\n",
      "Root word for mice --> mice\n",
      "Root word for ran --> ran\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "print('Porter Stemmer')\n",
    "print('Root word for cats -->', porter.stem('cats'))\n",
    "print('Root word for mice -->', porter.stem('mice'))\n",
    "print('Root word for ran -->', porter.stem('ran'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats --> cat\n",
      "mice --> mouse\n",
      "ran --> run\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'cats mice ran')\n",
    "for token in doc:\n",
    "    print(token, '-->', token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized = []\n",
    "c = []\n",
    "\n",
    "for word in tokenized[1]:\n",
    "    doc = nlp(word)\n",
    "    for token in doc:\n",
    "        c.append(token.lemma_)\n",
    "lemmatized.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from', 'guykuo', 'carson', 'u', 'washington', 'edu', 'guy', 'Kuo', 'subject', 'SI', 'Clock', 'poll', 'final', 'call', 'summary', 'final', 'call', 'for', 'SI', 'clock', 'report', 'keyword', 'SI', 'acceleration', 'clock', 'upgrade', 'article', 'I', 'd', 'shelley', 'qvfo', 'INNc', 's', 'organization', 'university', 'of', 'Washington', 'line', 'NNTP', 'post', 'host', 'carson', 'u', 'washington', 'edu', 'a', 'fair', 'number', 'of', 'brave', 'soul', 'who', 'upgrade', 'their', 'SI', 'clock', 'oscillator', 'have', 'share', 'their', 'experience', 'for', 'this', 'poll', 'please', 'send', 'a', 'brief', 'message', 'detail', 'your', 'experience', 'with', 'the', 'procedure', 'top', 'speed', 'attain', 'cpu', 'rate', 'speed', 'add', 'on', 'card', 'and', 'adapter', 'heat', 'sink', 'hour', 'of', 'usage', 'per', 'day', 'floppy', 'disk', 'functionality', 'with', 'and', 'm', 'floppy', 'be', 'especially', 'request', 'I', 'will', 'be', 'summarize', 'in', 'the', 'next', 'two', 'day', 'so', 'please', 'add', 'to', 'the', 'network', 'knowledge', 'base', 'if', 'you', 'have', 'do', 'the', 'clock', 'upgrade', 'and', 'haven', 't', 'answer', 'this', 'poll', 'thank', 'guy', 'Kuo', 'guykuo', 'u', 'washington', 'edu']\n"
     ]
    }
   ],
   "source": [
    "print(lemmatized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Stop Words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['guykuo', 'carson', 'u', 'washington', 'edu', 'guy', 'Kuo', 'subject', 'SI', 'Clock', 'poll', 'final', 'summary', 'final', 'SI', 'clock', 'report', 'keyword', 'SI', 'acceleration', 'clock', 'upgrade', 'article', 'I', 'd', 'shelley', 'qvfo', 'INNc', 's', 'organization', 'university', 'Washington', 'line', 'NNTP', 'post', 'host', 'carson', 'u', 'washington', 'edu', 'fair', 'number', 'brave', 'soul', 'upgrade', 'SI', 'clock', 'oscillator', 'share', 'experience', 'poll', 'send', 'brief', 'message', 'detail', 'experience', 'procedure', 'speed', 'attain', 'cpu', 'rate', 'speed', 'add', 'card', 'adapter', 'heat', 'sink', 'hour', 'usage', 'day', 'floppy', 'disk', 'functionality', 'm', 'floppy', 'especially', 'request', 'I', 'summarize', 'day', 'add', 'network', 'knowledge', 'base', 'clock', 'upgrade', 'haven', 't', 'answer', 'poll', 'thank', 'guy', 'Kuo', 'guykuo', 'u', 'washington', 'edu']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "stop_words = list(STOP_WORDS)\n",
    "\n",
    "stop_words_removed = []\n",
    "\n",
    "for word in lemmatized[0]:\n",
    "    if word not in stop_words:\n",
    "        stop_words_removed.append(word)\n",
    "        \n",
    "print(stop_words_removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Keyword Extraction via TF-IDF: a statistical technique that tells how important a word is to a document in a collection of documents\n",
    "\n",
    "#### TF(t,d) = count of t in d / number of words in d\n",
    "#### IDF(t)  = N / occurences of t in N documents\n",
    "#### TF-IDF = TF * IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          TF-IDF\n",
      "car     0.381339\n",
      "lerxst  0.353835\n",
      "wam     0.259709\n",
      "umd     0.211868\n",
      "tellme  0.176918\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(X[0].T.todense(), index=vectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "df = df.sort_values(\"TF-IDF\", ascending=False)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Word Embeddings via Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65109575\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import keyedvectors as word2vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model = word2vec.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "test = cosine_similarity(model['king'].reshape(1, -1), model['queen'].reshape(1, -1))[0][0]\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Reading data\n",
    "df = pd.read_csv(\"Twitter_Data.csv\")\n",
    "\n",
    "# Tokenizing data\n",
    "tokenized = []\n",
    "for text in df['clean_text'].values:\n",
    "    tokenized.append(word_tokenize(str(text)))\n",
    "    \n",
    "# Lemmatize data\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = []\n",
    "for text in tokenized:\n",
    "    c = []\n",
    "    for word in text:\n",
    "        c.append(lemmatizer.lemmatize(word))\n",
    "    lemmatized.append(c)\n",
    "    \n",
    "# Remove stop words\n",
    "stop_words_removed = []\n",
    "for text in lemmatized:\n",
    "    c = []\n",
    "    for word in text:\n",
    "        if word not in stop_words:\n",
    "            c.append(word)\n",
    "    stop_words_removed.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform text to embeddings\n",
    "vectors = []\n",
    "for text in stop_words_removed:\n",
    "    if text != []: # check if not all words were removed in stop word removal\n",
    "        temp = []\n",
    "        for word in text:\n",
    "            try:\n",
    "                temp.append(model[word])\n",
    "            except:\n",
    "                pass\n",
    "        if temp != []: # to check if embedding was found for at least one word in the tweet\n",
    "            vectors.append(np.average(np.array(temp), axis=0))\n",
    "        else:\n",
    "            vectors.append(np.zeros(300, dtype = np.float64))\n",
    "    else:\n",
    "        vectors.append(np.zeros(300, dtype = np.float64))\n",
    "\n",
    "vectors_cleaned = np.nan_to_num(vectors)\n",
    "y = df.category.values\n",
    "y = np.nan_to_num(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6578721315498834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\terenceshin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectors_cleaned, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create model\n",
    "clf = LogisticRegression(random_state=42).fit(np.array(X_train), np.array(y_train))\n",
    "predicted_y = clf.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "print(accuracy_score(y_test, predicted_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(stop_words_removed) # encapsulates the mapping between normalized words and their int IDs\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [dictionary.doc2bow(text) for text in stop_words_removed] # coverts words to bag of words format\n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus = corpus,\n",
    "                                            id2word = dictionary,\n",
    "                                            num_topics = 3,\n",
    "                                            random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\terenceshin\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyLDAvis.gensin'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-75a9711f571c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgensin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_notebook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mvis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis.gensin'"
     ]
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensin\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.prepare(lda_model, corpus, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "Japan is an island country in East Asia, located in the northwest Pacific Ocean.: 3.25, It is bordered on the west by the Sea of Japan, and extends from the Sea of Okhotsk in the north toward the East China Sea and Taiwan in the south.: 5.75, Part of the Ring of Fire, Japan spans an archipelago of 6852 islands covering 377,975 square kilometers (145,937 sq mi); \n",
      "the five main islands are Hokkaido, Honshu, Shikoku, Kyushu, and Okinawa.: 6.0, Tokyo is Japan's capital and largest city; other major cities include Yokohama, Osaka, Nagoya, Sapporo, Fukuoka, Kobe, and Kyoto.\n",
      ": 4.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\terenceshin\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from heapq import nlargest\n",
    "\n",
    "doc = \"\"\"\n",
    "Japan is an island country in East Asia, located in the northwest Pacific Ocean. \n",
    "It is bordered on the west by the Sea of Japan, and extends from the Sea of Okhotsk in the north toward the East China Sea and Taiwan in the south. \n",
    "Part of the Ring of Fire, Japan spans an archipelago of 6852 islands covering 377,975 square kilometers (145,937 sq mi); \n",
    "the five main islands are Hokkaido, Honshu, Shikoku, Kyushu, and Okinawa. \n",
    "Tokyo is Japan's capital and largest city; other major cities include Yokohama, Osaka, Nagoya, Sapporo, Fukuoka, Kobe, and Kyoto.\n",
    "\"\"\"\n",
    "\n",
    "doc = nlp(doc)\n",
    "\n",
    "keyword = []\n",
    "pos_tag = ['PROPN','ADJ','NOUN','VERB']\n",
    "for token in doc:\n",
    "    if(token.text in stop_words or token.text in punctuation):\n",
    "        continue\n",
    "    if (token.pos_ in pos_tag):\n",
    "        keyword.append(token.text)\n",
    "\n",
    "freq_word = Counter(keyword)\n",
    "\n",
    "max_freq = Counter(keyword).most_common(1)[0][1]\n",
    "for word in freq_word.keys():\n",
    "    freq_word[word] = (freq_word[word]/max_freq)\n",
    "    \n",
    "sent_strength = {}\n",
    "for sent in doc.sents:\n",
    "    for word in sent:\n",
    "        if word.text in freq_word.keys():\n",
    "            if sent in sent_strength.keys():\n",
    "                sent_strength[sent] += freq_word[word.text]\n",
    "            else:\n",
    "                sent_strength[sent] = freq_word[word.text]\n",
    "\n",
    "print(sent_strength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Japan', 'GPE')\n",
      "('East Asia', 'LOC')\n",
      "('Pacific Ocean', 'LOC')\n",
      "('the Sea of Japan', 'LOC')\n",
      "('the Sea of Okhotsk', 'LOC')\n",
      "('the East China Sea', 'LOC')\n",
      "('Taiwan', 'GPE')\n",
      "('Japan', 'GPE')\n",
      "('6852', 'CARDINAL')\n",
      "('377,975 square kilometers', 'QUANTITY')\n",
      "('145,937', 'CARDINAL')\n",
      "('five', 'CARDINAL')\n",
      "('Hokkaido', 'ORG')\n",
      "('Honshu', 'GPE')\n",
      "('Shikoku', 'LOC')\n",
      "('Kyushu', 'GPE')\n",
      "('Okinawa', 'PERSON')\n",
      "('Tokyo', 'GPE')\n",
      "('Japan', 'GPE')\n",
      "('Yokohama', 'GPE')\n",
      "('Osaka', 'GPE')\n",
      "('Nagoya', 'GPE')\n",
      "('Sapporo', 'GPE')\n",
      "('Fukuoka', 'GPE')\n",
      "('Kobe', 'GPE')\n",
      "('Kyoto', 'GPE')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\terenceshin\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "doc = \"\"\"\n",
    "Japan is an island country in East Asia, located in the northwest Pacific Ocean. \n",
    "It is bordered on the west by the Sea of Japan, and extends from the Sea of Okhotsk in the north toward the East China Sea and Taiwan in the south. \n",
    "Part of the Ring of Fire, Japan spans an archipelago of 6852 islands covering 377,975 square kilometers (145,937 sq mi); \n",
    "the five main islands are Hokkaido, Honshu, Shikoku, Kyushu, and Okinawa. \n",
    "Tokyo is Japan's capital and largest city; other major cities include Yokohama, Osaka, Nagoya, Sapporo, Fukuoka, Kobe, and Kyoto.\n",
    "\"\"\"\n",
    "\n",
    "doc = nlp(doc)\n",
    "\n",
    "entities = [(X.text, X.label_) for X in doc.ents]\n",
    "for entity in entities:\n",
    "    print(entity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
